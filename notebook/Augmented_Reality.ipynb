{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Reality \n",
    "\n",
    "This notebook aims to give a brief explaination of the theory and math behind the present *augmented reality* project.  \n",
    "The content is based on the articles ([part1](https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/) and [part2](https://bitesofcode.wordpress.com/2018/09/16/augmented-reality-with-python-and-opencv-part-2/)) written by *Juan Gallostra*, while the code that you can find in this project is a fork from his [project](https://github.com/juangallostra/augmented-reality) too.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required module for the notebook\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other interesting articles:  \n",
    "[glyph recognition](https://rdmilligan.wordpress.com/2015/07/19/glyph-recognition-using-opencv-and-python/)  \n",
    "[AR with python OpenCV and OpenGL](https://rdmilligan.wordpress.com/2015/10/15/augmented-reality-using-opencv-opengl-and-blender/)  \n",
    "[pygame OBJ loader](http://www.pygame.org/wiki/OBJFileLoader)  \n",
    "[3D object search](https://clara.io/library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of the project is to display in a screen a 3D model of a figure whose position and orientation matches the position and orientation of some predefined flat surface. Furthermore, we want to do it in real time, so that if the surface changes its position or orientation the projected model does so accordingly.  \n",
    "\n",
    "To achieve this we first have to be able to identify the flat surface of reference in an image or video frame. Once identified, we can easily determine the transformation from the reference surface image (2D) to the target image (2D). This transformation is called [homography](https://en.wikipedia.org/wiki/Homography_(computer_vision)). However, if what we want is to project a 3D model placed on top of the reference surface to the target image we need to extend the previous transformation to handle cases were the height of the point to project in the reference surface coordinate system is different than zero. This can be achieved with a bit of algebra. Finally, we should apply this transformation to our 3D model and draw it on the screen. Bearing the previous points in mind our project can be divided into:\n",
    "\n",
    "1.  [Recognize the reference flat surface.](#1.-Recognition-of-the-reference-target)\n",
    "2.  [Estimate the homography.](#2.-Homography-estimation)\n",
    "3.  [Derive from the homography the transformation (pose estimation) from the reference surface coordinate system to the target image coordinate system.](#3.-Pose-estimation-from-a-plane)\n",
    "4.  [Project our 3D model in the image (pixel space) and draw it.](#4.-Model-projection)\n",
    "          \n",
    "The main tools we will use are *Python* and *OpenCV* because they are both open source, easy to set up and use and it is fast to build prototypes with them. For the needed algebra bit I will be using *numpy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**insert image of 1.2.3.4. scheme here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recognition of the reference target\n",
    "\n",
    "From the many possible techniques that exist to perform object recognition I decided to tackle the problem with a feature based recognition method. This kind of methods, without going into much detail, consist in three main steps: \n",
    "\n",
    "1. [feature detection or extraction.](#1.1.-Feature-extraction)\n",
    "2. [feature description.](#1.2.-Feature-description)\n",
    "3. [feature matching.](#1.3.-Feature-matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Feature extraction\n",
    "\n",
    "Roughly speaking, this step consists in first looking in both the reference and target images for features that stand out and, in some way, describe part the object to be recognized. This features can be later used to find the reference object in the target image. We will assume we have found the object when a certain number of positive feature matches are found between the target and reference images. For this to work it is important to have a reference image where the only thing seen is the object (or surface, in this case) to be found.  We don’t want to detect features that are not part of the surface. And, although we will deal with this later, we will use the dimensions of the reference image when estimating the pose of the surface in a scene.  \n",
    "\n",
    "For a region or point of an image to be labeled as feature it should fulfill two important properties: first of all, it should present some uniqueness at least locally. Good examples of this could be corners or edges. Secondly, since we don’t know beforehand which will be, for example, the orientation, scale or brightness conditions of this same object in the image where we want to recognize it a feature should, ideally, be invariant to transformations; i.e, invariant against scale, rotation or brightness changes. As a rule of thumb, the more invariant the better.\n",
    "\n",
    "<img align=\"left\" height=300 width=300 src=\"../reference/glyph_01.png\" />\n",
    "<img align=\"left\" height=300 width=300 src=\"images/image_features.png\" />  \n",
    "\n",
    "*Example of features extracted (right) from a reference target (left).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. [Feature description](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html)\n",
    "\n",
    "Once features have been found we should find a suitable representation of the information they provide. This will allow us to look for them in other images and also to obtain a measure of how similar two detected features are when being compared. This is were descriptors roll in. A descriptor provides a representation of the information given by a feature and its surroundings. Once the descriptors have been computed the object to be recognized can then be abstracted to a [feature vector](https://en.wikipedia.org/wiki/Feature_(machine_learning)),  which is a vector that contains the descriptors of the keypoints found in the image with the reference object.  \n",
    "\n",
    "This is for sure a nice idea, but how can it actually be done? There are many algorithms that extract image features and compute its descriptors and, since I won’t go into much more detail (a whole post could be devoted only to this) if you are interested in knowing more take a look at [SIFT](http://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/), [SURF](http://www.vision.ee.ethz.ch/~surf/eccv06.pdf), or [Harris](http://aishack.in/tutorials/harris-corner-detector/). The one we will be using was developed at the OpenCV Lab and it is called [ORB](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html) (Oriented FAST and Rotated BRIEF). The shape and values of the descriptor depend on the algorithm used and, in our case,  the descriptors obtained will be binary strings.  \n",
    "\n",
    "With *OpenCV*, extracting features and its descriptors via the ORB detector is as easy as:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../reference/glyph_01.png', 0)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# find the keypoints with ORB\n",
    "kp = orb.detect(img, None)\n",
    "\n",
    "# compute the descriptors with ORB\n",
    "kp, des = orb.compute(img, kp)\n",
    "\n",
    "# draw keypoints location, size and orientation\n",
    "img2 = cv2.drawKeypoints(img, kp, img, color=(0,255,0), flags=4)\n",
    "\n",
    "# resize image\n",
    "width  = int(img2.shape[1] * 0.8)\n",
    "height = int(img2.shape[0] * 0.8)\n",
    "img3 = cv2.resize(img2, (width, height))\n",
    "\n",
    "# display image\n",
    "cv2.imshow('keypoints', img3)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. [Feature matching](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html)\n",
    "\n",
    "Once we have found the features of both the object and the scene were the object is to be found and computed its descriptors it is time to look for matches between them. The simplest way of doing this is to take the descriptor of each feature in the first set, compute the distance to all the descriptors in the second set and return the closest one as the best match (I should state here that it is important to choose a way of measuring distances suitable with the descriptors being used. Since our descriptors will be binary strings we will use [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)). This is a brute force approach, and more sophisticated methods exist.\n",
    "\n",
    "For example, and this is what we will be also using, we could check that the match found as explained before is also the best match when computing matches the other way around, from features in the second set to features in the first set. This means that both features match each other. Once the matching has finished in both directions we will take as valid matches only the ones that fulfilled the previous condition.\n",
    "\n",
    "Another option to reduce the number of false positives would be to check if the distance to the second to best match is below a certain threshold.  If it is, then the match is considered valid.\n",
    "\n",
    "**image**\n",
    "\n",
    "Finally, after matches have been found, we should define some criteria to decide if the object has been found or not. For this I defined a threshold on the minimum number of matches that should be found. If the number of matches is above the threshold, then we assume the object has been found. Otherwise we consider that there isn’t enough evidence to say that the recognition was successful.\n",
    "\n",
    "With OpenCV all this recognition process can be done in a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCHES = 30\n",
    "\n",
    "# read images in grayscale mode\n",
    "cap   = cv2.imread('images/scene_ticket.jpg', 0)    \n",
    "model = cv2.imread('../reference/ticket.jpg', 0)\n",
    "\n",
    "# ORB keypoint detector\n",
    "orb = cv2.ORB_create()              \n",
    "# create brute force  matcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  \n",
    "\n",
    "# Compute model keypoints and its descriptors\n",
    "kp_model, des_model = orb.detectAndCompute(model, None)  \n",
    "# Compute scene keypoints and its descriptors\n",
    "kp_frame, des_frame = orb.detectAndCompute(cap, None)\n",
    "\n",
    "# Match frame descriptors with model descriptors\n",
    "matches = bf.match(des_model, des_frame)\n",
    "# Sort them in the order of their distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "if len(matches) > MIN_MATCHES:\n",
    "    # draw first 15 matches.\n",
    "    cap = cv2.drawMatches(model, kp_model, cap, kp_frame,\n",
    "                          matches[:MIN_MATCHES], 0, flags=2)\n",
    "    # resize\n",
    "    width  = int(cap.shape[1] * 0.3)\n",
    "    height = int(cap.shape[0] * 0.3)\n",
    "    cap2 = cv2.resize(cap, (width, height))\n",
    "    # show result\n",
    "    cv2.imshow('frame', cap2)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"Not enough matches found - %d/%d\" % (len(matches), MIN_MATCHES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a final note and before stepping into the next step of the process I must point out that, since we want a real time application, it would have been better to implement a tracking technique and not just plain recognition. This is due to the fact that object recognition will be performed in each frame independently without taking into account previous frames that could add valuable information about the location of the reference object. Another thing to take into account is that, the easier to found the reference surface the more robust detection will be. In this particular sense, the reference surface I’m using might not be the best option, but it helps to understand the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [Homography estimation](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_feature_homography/py_feature_homography.html)\n",
    "\n",
    "Once we have identified the reference surface in the current frame and have a set of valid matches we can proceed to estimate the homography between both images. As explained before, we want to find the transformation that maps points from the target surface plane to the image plane. This transformation will have to be updated each new frame we process.  \n",
    "\n",
    "Since we have already found a set of matches between both images we can certainly find directly by any of the existing methods (I advance we will be using [RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus)) an homogeneous transformation that performs the mapping.\n",
    "\n",
    "What we have is an object (a plane in this case) with known coordinates in the *World* coordinate system and we take a picture of it with a camera located at a certain position and orientation with respect to the World coordinate system. We will assume the camera works following the [pinhole model](https://en.wikipedia.org/wiki/Pinhole_camera_model), which roughly means that the rays passing through a 3D point *p* and the corresponding 2D point *u* intersect at *c*, the camera center. A good resource if you are interested in knowing more about the pinhole model can be found [here](http://alumni.media.mit.edu/~maov/classes/comp_photo_vision08f/lect/09_image_formation.pdf).  \n",
    "\n",
    "Although not entirely true, the pinhole model assumption eases our calculations and works well enough for our purposes. The *u*, *v* coordinates (coordinates in the image plane) of a point *p* expressed in the Camera coordinate system if we assume a pinhole camera can be computed as (the derivation of the equation is left as an exercise to the reader):\n",
    "\n",
    "$$  \n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        u \\cdot k \\\\\n",
    "        v \\cdot k \\\\\n",
    "        k\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    A\n",
    "    \\begin{bmatrix}\n",
    "        x_{cam} \\\\\n",
    "        y_{cam} \\\\\n",
    "        z_{cam}\n",
    "    \\end{bmatrix}\n",
    "    \\label{eq:pin_hole_model}\n",
    "    \\tag{1}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "where the matrix \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    A =\n",
    "    \\begin{bmatrix}\n",
    "        f_u & 0   & u_0\\\\\n",
    "        0   & f_v & v_0\\\\\n",
    "        0   & 0   & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "is know as *calibration matrix*, $(u_0, v_0)$ are the *projection of the optical center*, which is the position of the optical center in the image plane, $k$ is a scaling factor and $f_u, f_v$ are the *focal lengths*, the distance from the pinhole to the image plane.\n",
    "\n",
    "Equation ($\\ref{eq:pin_hole_model}$) tells us how the image is formed. However, as stated before, we know the coordinates of the point *p* in the World coordinate system and not in the Camera coordinate system, so we have to add another transformation that maps points from the World coordinate system to the Camera coordinate system. The transformation that tells us the coordinates in the image plane of a point *p* in the World coordinate system is then:  \n",
    "\n",
    "$$  \n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        u \\cdot k \\\\\n",
    "        v \\cdot k \\\\\n",
    "        k\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    A \n",
    "    \\begin{bmatrix}\n",
    "        R & t\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x_{world} \\\\\n",
    "        y_{world} \\\\\n",
    "        z_{world} \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "    \\label{eq:pin_hole_model_extended}\n",
    "    \\tag{2}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "Where $R$ is a $3 \\times 3$ rotation matrix and $t$ is the translation vector that specifies the position of the reference target.\n",
    "\n",
    "Luckily for us, since the points in the reference surface plane do always have its *z* coordinate equal to 0 (it is a surface!) we can simplify the transformation that we found above. It can be easily seen that the product of the *z* coordinate and the third column of the projection matrix will always be $0$ so we can drop this column and the *z* coordinate from the previous equation. \n",
    "\n",
    "$$  \n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        u \\cdot k \\\\\n",
    "        v \\cdot k \\\\\n",
    "        k\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    A \n",
    "    \\begin{bmatrix}\n",
    "        R_1 & R_2 & R_3 & t\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x_{world} \\\\\n",
    "        y_{world} \\\\\n",
    "        0 \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    A\n",
    "    \\begin{bmatrix}\n",
    "        R_1 & R_2 & t\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x_{world} \\\\\n",
    "        y_{world} \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "    = \n",
    "    H\n",
    "    \\begin{bmatrix}\n",
    "        x_{world} \\\\\n",
    "        y_{world} \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "    \\label{eq:pin_hole_model_simplified}\n",
    "    \\tag{3}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "There are several methods that allow us to estimate the values of the homography matrix $H$, and you might be familiar with some of them. The one we will be using is **RANdom SAmple Consensus** ([RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus)).  RANSAC is an iterative algorithm used for model fitting in the presence of a large number of outliers and can be schematize as follows.\n",
    "\n",
    "1. Choose a small subset of points uniformly at random.\n",
    "2. Fit a model to that subset.\n",
    "3. Find all remaining points that are \"close\" to the model and reject the rest as outliers.\n",
    "4. Do this many times and choose the best model.\n",
    "\n",
    "Since we cannot guarantee that all the matches we have found are actually valid matches we have to consider that there might be some false matches (which will be our outliers) and, hence, we have to use an estimation method that is robust against outliers.\n",
    "\n",
    "For homography estimation the algorithm can be outlined with the following steps:\n",
    "\n",
    "1. Randomly sample 4 matches.\n",
    "2. Estimate Homography H.\n",
    "3. Verify homography: search for other matches consistent with H.\n",
    "4. Iterate until convergence.\n",
    "\n",
    "Before seeing how OpenCV can handle this for us we should  discuss one final aspect of the algorithm, which is what does it mean that a match is consistent with $H$. What this mainly means is that if after estimating an homography we project into the target image the matches that were not used to estimate it then the projected points from the reference surface should be close to its matches in the target image. How close they should be to be considered consistent is up to you.\n",
    "In OpenCV estimating the homography with RANSAC is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming matches stores the matches found and \n",
    "# returned by bf.match(des_model, des_frame)\n",
    "# differenciate between source points and destination points\n",
    "src_pts = np.float32([kp_model[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([kp_frame[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "# compute Homography\n",
    "M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where 5.0 is the threshold distance to determine if a match is consistent with the estimated homography. If after estimating the homography we project the four corners of the reference surface on the target image and connect them with a line we should expect the resulting lines to enclose the reference surface in the target image. We can do this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv2.imread('../reference/ticket.jpg', 0)\n",
    "frame = cv2.imread('images/scene_ticket.jpg', 0)    \n",
    "# Draw a rectangle that marks the found model in the frame\n",
    "h, w = model.shape\n",
    "pts = np.float32([[0, 0], [0, h - 1], \n",
    "                  [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "# project corners into frame\n",
    "dst = cv2.perspectiveTransform(pts, M)  \n",
    "# connect them with lines\n",
    "img = cv2.polylines(frame, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "img2 = cv2.drawMatches(model, kp_model, img, kp_frame, matches[:MIN_MATCHES], 0, flags=2)\n",
    "# resize\n",
    "width  = int(img2.shape[1] * 0.4)\n",
    "height = int(img2.shape[0] * 0.4)\n",
    "img2 = cv2.resize(img2, (width, height))\n",
    "# show\n",
    "cv2.imshow('frame', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pose estimation from a plane\n",
    "\n",
    "To locate the 3D model in the camera frame we need its pose estimation with respect the camera frame, that is a rotation matrix $R$ and a position vector $t$, that is a homogeneus transformation from points in the *World* frame to the *Camera* frame.  \n",
    "This values are the same of equation ($\\ref{eq:pin_hole_model_simplified}$), and since at this point the homography matrix $H$ has been already estimated we could invert equation ($\\ref{eq:pin_hole_model_simplified}$) and obtain: \n",
    "\n",
    "$$  \n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        R_1 & R_2 & t\n",
    "    \\end{bmatrix}\n",
    "    = \n",
    "    A^{-1}H\n",
    "    \\label{eq:homogeneus_transformation}\n",
    "    \\tag{4}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "Equation ($\\ref{eq:homogeneus_transformation}$) has still some problem.  \n",
    "First of all matrix $A$ is unknown, there are methods to evaluate an estimation (see [this](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html), [this](https://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/) and [this](http://ksimek.github.io/2012/08/13/introduction/)), but for now the code assume that the matrix is known *a priori* with some \"rational\" values.  \n",
    "Another problem is the absence of $R_3$, which is the estmate of the *z-axis* of the *World* frame. So it is necessary to add a step to evalluate $R_3$. It can be evaluated as $R_1 \\times R_2$, however the values of $R_1$ and $R_2$ are affected by some estimation error and they can not result to be *orthonormal*, so its needed to correct them too.  \n",
    "\n",
    "*Remark: the correction of $R_1$ and $R_2$ is not important to the estimate of $R_3$ but for the representation of the entire matrix $R$! if $R_1$, $R_2$ and $R_3$ are not orthonormal the transformation $R$ rotates and **deforms** the points transformated resulting in the rotation and **deformation** of the 3D object rendered!*\n",
    "\n",
    "The estimation of the basis considers the assumption that, since $R_1$ and $R_2$ are estimates of the real ${R_1}'$ and ${R_2}'$ (which are orthonormal), the angle between $R_1$ and $R_2$ will be not 90 degrees. Furthermore, the modulus of each of this vectors will be close to 1. From $R_1$ and $R_2$ we can easily compute an orthogonal basis -this meaning that the angle between the basis vectors will exactly be 90 degrees- that will be rotated approximately 45° degrees clockwise with respect to the basis formed by $R_1$ and $R_2$. This basis is the one formed by $c=R_1+R_2$ and  $d=c \\times p=(R_1+R_2)\\times (R_1\\times R_2)$ in figure below. If the vectors that form this new basis $(c,d)$ are made unit vectors and rotated 45° degrees counterclockwise (note that once the vectors have been transformed into unit vectors rotating the basis is as easy as d’ = c / ||c|| + d / ||d|| and  c’ = c / ||c|| – d / ||d||), guess what? We will have an orthogonal basis which is pretty close to our original basis $(R_1,R_2)$. If we normalize this rotated basis we will finally get the pair of vectors we were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" height=600 width=400 src=\"images/img_notebook_1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this basis $({R_1}',{R_2}')$ has been obtained it is trivial to get the value of $R_3$ as $R_1' \\times R_2'$.  \n",
    "Below you can find the function which compute the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_matrix(camera_parameters, homography):\n",
    "\"\"\"\n",
    " From the camera calibration matrix and the estimated homography\n",
    " compute the 3D projection matrix\n",
    " \"\"\"\n",
    "# Compute rotation along the x and y axis as well as the translation\n",
    "homography = homography * (-1)\n",
    "rot_and_transl = np.dot(np.linalg.inv(camera_parameters), homography)\n",
    "col_1 = rot_and_transl[:, 0]\n",
    "col_2 = rot_and_transl[:, 1]\n",
    "col_3 = rot_and_transl[:, 2]\n",
    "# normalise vectors\n",
    "l = math.sqrt(np.linalg.norm(col_1, 2) * np.linalg.norm(col_2, 2))\n",
    "rot_1 = col_1 / l\n",
    "rot_2 = col_2 / l\n",
    "translation = col_3 / l\n",
    "# compute the orthonormal basis\n",
    "c = rot_1 + rot_2\n",
    "p = np.cross(rot_1, rot_2)\n",
    "d = np.cross(c, p)\n",
    "rot_1 = np.dot(c / np.linalg.norm(c, 2) + d / np.linalg.norm(d, 2), 1 / math.sqrt(2))\n",
    "rot_2 = np.dot(c / np.linalg.norm(c, 2) - d / np.linalg.norm(d, 2), 1 / math.sqrt(2))\n",
    "rot_3 = np.cross(rot_1, rot_2)\n",
    "# finally, compute the 3D projection matrix from the model to the current frame\n",
    "projection = np.stack((rot_1, rot_2, rot_3, translation)).T\n",
    "return np.dot(camera_parameters, projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model projection\n",
    "\n",
    "The program currently only uses simple models in `.obj` format, because they are easy to process and render directly with bare Python without having to make use of other libraries such as OpenGL. The problem with complex models is that the amount of processing they require is way more than what my computer can handle. Since I want my application to be real-time, this limits the complexity of the models I am able to render.  \n",
    "\n",
    "The code I used to load the models is based on this [OBJFileLoader](http://www.pygame.org/wiki/OBJFileLoader) script that I found on Pygame’s website. I stripped out any references to OpenGL and left only the code that loads the geometry of the model. Once the model is loaded we just have to implement a function that reads this data and projects it on top of the video frame with the projection matrix we obtained in the previous section. To do so we take every point used to define the model and multiply it by the projection matrix. One this has been done, we only have to fill with color the faces of the model. The following function can be used to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(img, obj, projection, model, color=False):\n",
    "    vertices = obj.vertices\n",
    "    scale_matrix = np.eye(3) * 3\n",
    "    h, w = model.shape\n",
    "\n",
    "    for face in obj.faces:\n",
    "        face_vertices = face[0]\n",
    "        points = np.array([vertices[vertex - 1] for vertex in face_vertices])\n",
    "        points = np.dot(points, scale_matrix)\n",
    "        # render model in the middle of the reference surface. To do so,\n",
    "        # model points must be displaced\n",
    "        points = np.array([[p[0] + w / 2, p[1] + h / 2, p[2]] for p in points])\n",
    "        dst = cv2.perspectiveTransform(points.reshape(-1, 1, 3), projection)\n",
    "        imgpts = np.int32(dst)\n",
    "        if color is False:\n",
    "            cv2.fillConvexPoly(img, imgpts, (137, 27, 211))\n",
    "        else:\n",
    "            color = hex_to_rgb(face[-1])\n",
    "            color = color[::-1] # reverse\n",
    "            cv2.fillConvexPoly(img, imgpts, color)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to be highlighted from the previous function:\n",
    "\n",
    "1. The scale factor: Since we don’t know the actual size of the model with respect to the rest of the frame, we may have to scale it (manually for now) so that it haves the desired size. The scale matrix allows us to resize the model.\n",
    "2. I like the model to be rendered on the middle of the reference surface frame. However, the reference frame of the models is located at the center of the model. This means that if we project directly the points of the OBJ model in the video frame our model will be rendered on one corner of the reference surface. To locate the model in the middle of the reference surface we have to, before projecting the points on the video frame, displace the x and y coordinates of all the model points by half the width and height of the reference surface.\n",
    "3. There is an optional color parameter than can be set to True. This is because some models also have color information that can be used to color the different faces of the model. I didn’t test it enough and setting it to True might result in some problems. It is not 100% guaranteed this feature will work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
